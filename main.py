import requests
import json
import os
import mysql.connector
import numpy as np
import pandas as pd
import pandas_ta as ta
from datetime import datetime
import time
import sys
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Force stdout to be unbuffered
sys.stdout.reconfigure(line_buffering=True)

# Load environment variables for security
TAAPI_KEY = os.getenv('TAAPI_KEY')
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
MYSQL_HOST = os.getenv('MYSQL_HOST')
MYSQL_USER = os.getenv('MYSQL_USER')
MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD')
MYSQL_DB = os.getenv('MYSQL_DB')

if not all([TAAPI_KEY, TELEGRAM_TOKEN, TELEGRAM_CHAT_ID, MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DB]):
    logger.error("Missing required environment variables.")
    raise ValueError("Missing required environment variables.")

logger.info("Starting crypto trend bot...")
logger.info(f"Monitoring symbols: BTC/USDT, ETH/USDT")
logger.info(f"MySQL Host: {MYSQL_HOST}")
logger.info(f"Telegram Chat ID: {TELEGRAM_CHAT_ID}")

# Test database connection
try:
    conn = mysql.connector.connect(
        host=MYSQL_HOST,
        user=MYSQL_USER,
        password=MYSQL_PASSWORD,
        database=MYSQL_DB
    )
    conn.close()
    logger.info("âœ“ Database connection successful")
except Exception as e:
    logger.error(f"âœ— Database connection failed: {str(e)}")
    logger.error("Please check your database configuration and ensure MySQL is running")
    sys.exit(1)

def fetch_taapi_data(symbol, interval="1h", max_retries=3):
    url = "https://api.taapi.io/bulk"
    payload = {
        "secret": TAAPI_KEY,
        "construct": {
            "exchange": "binance",
            "symbol": symbol,
            "interval": interval,
            "indicators": [
                {"indicator": "sma", "period": 50, "id": "sma50"},
                {"indicator": "sma", "period": 200, "id": "sma200"},
                {"indicator": "dmi", "id": "dmi"},  # Default period 14 for ADX, +DI, -DI
                {"indicator": "bbands", "results": 20, "id": "bbands"},  # Limited to 20 results
                {"indicator": "atr", "results": 20, "id": "atr"},  # Limited to 20 results
                {"indicator": "price", "id": "price"}  # Get current price data
            ]
        }
    }
    
    for attempt in range(max_retries):
        try:
            logger.debug(f"Fetching data for {symbol} (attempt {attempt + 1}/{max_retries})")
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                logger.debug(f"Successfully fetched data for {symbol}")
                return response.json()['data']
            elif response.status_code == 429:  # Rate limit
                wait_time = (attempt + 1) * 10  # Exponential backoff
                logger.warning(f"Rate limit hit for {symbol}, waiting {wait_time} seconds...")
                time.sleep(wait_time)
                continue
            else:
                logger.error(f"TAAPI error for {symbol}: {response.text}")
                if attempt == max_retries - 1:  # Last attempt
                    raise Exception(f"TAAPI error for {symbol}: {response.text}")
                time.sleep(5)  # Wait before retry
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error for {symbol}: {str(e)}")
            if attempt == max_retries - 1:  # Last attempt
                raise Exception(f"Request error for {symbol}: {str(e)}")
            time.sleep(5)  # Wait before retry
    
    raise Exception(f"Failed to fetch data for {symbol} after {max_retries} attempts")

def parse_indicators(data):
    indicators = {}
    for item in data:
        ind_id = item['id']
        result = item['result']
        if isinstance(result, list):
            # For historical data (bbands, atr), assume ordered oldest to newest, last is current
            if ind_id == 'bbands':
                bandwidths = [
                    (r['valueUpperBand'] - r['valueLowerBand']) / r['valueMiddleBand'] * 100
                    if r['valueMiddleBand'] != 0 else 0 for r in result
                ]
                indicators['bandwidths'] = bandwidths
            elif ind_id == 'atr':
                atr_values = [r['value'] for r in result]
                indicators['atr_values'] = atr_values
        else:
            # For single results
            if ind_id in ['sma50', 'sma200']:
                indicators[ind_id] = result['value']
            elif ind_id == 'dmi':
                indicators['adx'] = result['adx']
                indicators['pdi'] = result['pdi']
                indicators['mdi'] = result['mdi']
            elif ind_id == 'price':
                indicators['price'] = result['value']
                indicators['open'] = result.get('open', result['value'])
                indicators['high'] = result.get('high', result['value'])
                indicators['low'] = result.get('low', result['value'])
                indicators['close'] = result['value']
                indicators['volume'] = result.get('volume', 0)
    return indicators

def get_historical_data(symbol, minutes_back):
    """ä»æ•°æ®åº“è·å–å†å²æ•°æ®ç”¨äºå¤šæ—¶é—´æ¡†æ¶åˆ†æ"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT timestamp, price, adx, pdi, mdi, sma50, sma200, bandwidth, atr
            FROM crypto_5min_data 
            WHERE symbol = %s AND timestamp >= DATE_SUB(NOW(), INTERVAL %s MINUTE)
            ORDER BY timestamp DESC
            LIMIT %s
        """, (symbol, minutes_back, minutes_back // 5))  # 5åˆ†é’Ÿæ•°æ®ï¼Œæ‰€ä»¥é™¤ä»¥5
        
        data = cursor.fetchall()
        cursor.close()
        conn.close()
        
        return data
    except Exception as e:
        logger.error(f"Error fetching historical data for {symbol}: {str(e)}")
        return []

def determine_trend(indicators, timeframe="5m"):
    """åŸºäºæŒ‡æ ‡å’Œæ—¶é—´æ¡†æ¶åˆ¤æ–­è¶‹åŠ¿"""
    adx = indicators['adx']
    pdi = indicators['pdi']
    mdi = indicators['mdi']
    sma50 = indicators['sma50']
    sma200 = indicators['sma200']
    
    logger.info(f"[{timeframe}] Indicators - ADX: {adx:.2f}, +DI: {pdi:.2f}, -DI: {mdi:.2f}, SMA50: {sma50:.2f}, SMA200: {sma200:.2f}")
    
    # æ ¹æ®æ—¶é—´æ¡†æ¶è°ƒæ•´é˜ˆå€¼
    adx_strong_threshold = 25
    adx_moderate_threshold = 20
    sma_diff_threshold = 0.02
    
    if timeframe == "15m":
        adx_strong_threshold = 35  # 15åˆ†é’Ÿéœ€è¦æ›´å¼ºçš„ä¿¡å·
        adx_moderate_threshold = 30
        sma_diff_threshold = 0.008
    elif timeframe == "1h":
        adx_strong_threshold = 30  # 1å°æ—¶éœ€è¦æ›´å¼ºçš„ä¿¡å·
        adx_moderate_threshold = 25
        sma_diff_threshold = 0.01
    elif timeframe == "4h":
        adx_strong_threshold = 25
        adx_moderate_threshold = 20
        sma_diff_threshold = 0.015
    elif timeframe == "1d":
        adx_strong_threshold = 20  # é•¿æœŸæ—¶é—´æ¡†æ¶å¯ä»¥ç”¨è¾ƒä½é˜ˆå€¼
        adx_moderate_threshold = 15
        sma_diff_threshold = 0.02
    elif timeframe == "1w":
        adx_strong_threshold = 18
        adx_moderate_threshold = 12
        sma_diff_threshold = 0.03
    
    # Bandwidth analysis
    bandwidths = indicators.get('bandwidths', [])
    if len(bandwidths) >= 10:
        avg_recent_bw = np.mean(bandwidths[-5:]) if len(bandwidths) >= 5 else np.mean(bandwidths)
        percentile_30_bw = np.percentile(bandwidths, 30)
        low_bw = avg_recent_bw < percentile_30_bw
    else:
        low_bw = False
    
    # ATR analysis
    atr_values = indicators.get('atr_values', [])
    if len(atr_values) >= 10:
        current_atr = atr_values[-1]
        percentile_30_atr = np.percentile(atr_values, 30)
        low_atr = current_atr < percentile_30_atr
    else:
        low_atr = False
    
    logger.info(f"[{timeframe}] Trend analysis - ADX>{adx_strong_threshold}: {adx > adx_strong_threshold}, +DI>-DI: {pdi > mdi}, SMA50>SMA200: {sma50 > sma200}")
    
    # è¶‹åŠ¿åˆ¤æ–­é€»è¾‘
    if adx > adx_strong_threshold and pdi > mdi and sma50 > sma200:
        logger.info(f"[{timeframe}] Trend decision: ä¸Šæ¶¨è¶‹åŠ¿ (Strong uptrend)")
        return "ä¸Šæ¶¨è¶‹åŠ¿"
    elif adx > adx_strong_threshold and mdi > pdi and sma50 < sma200:
        logger.info(f"[{timeframe}] Trend decision: ä¸‹è·Œè¶‹åŠ¿ (Strong downtrend)")
        return "ä¸‹è·Œè¶‹åŠ¿"
    elif adx > adx_moderate_threshold and pdi > mdi * 1.1 and sma50 > sma200:
        logger.info(f"[{timeframe}] Trend decision: ä¸Šæ¶¨è¶‹åŠ¿ (Moderate uptrend)")
        return "ä¸Šæ¶¨è¶‹åŠ¿"
    elif adx > adx_moderate_threshold and mdi > pdi * 1.1 and sma50 < sma200:
        logger.info(f"[{timeframe}] Trend decision: ä¸‹è·Œè¶‹åŠ¿ (Moderate downtrend)")
        return "ä¸‹è·Œè¶‹åŠ¿"
    elif adx <= adx_strong_threshold and (low_bw or low_atr):
        logger.info(f"[{timeframe}] Trend decision: åŒºé—´/æ³¢åŠ¨å° (Range-bound)")
        return "åŒºé—´/æ³¢åŠ¨å°"
    elif sma50 > sma200 * (1 + sma_diff_threshold):
        logger.info(f"[{timeframe}] Trend decision: ä¸Šæ¶¨è¶‹åŠ¿ (SMA-based uptrend)")
        return "ä¸Šæ¶¨è¶‹åŠ¿"
    elif sma50 < sma200 * (1 - sma_diff_threshold):
        logger.info(f"[{timeframe}] Trend decision: ä¸‹è·Œè¶‹åŠ¿ (SMA-based downtrend)")
        return "ä¸‹è·Œè¶‹åŠ¿"
    else:
        logger.info(f"[{timeframe}] Trend decision: æœªçŸ¥ (Mixed signals)")
        return "æœªçŸ¥"

def calculate_indicators_from_5min_data(symbol, timeframe_minutes):
    """åŸºäº5åˆ†é’Ÿæ•°æ®è®¡ç®—æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æŠ€æœ¯æŒ‡æ ‡"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # è®¡ç®—éœ€è¦çš„5åˆ†é’Ÿæ•°æ®é‡
        # ä¸ºäº†è®¡ç®—SMA200ï¼Œæˆ‘ä»¬éœ€è¦200ä¸ªç›®æ ‡æ—¶é—´æ¡†æ¶çš„å‘¨æœŸ
        # æ¯ä¸ªç›®æ ‡æ—¶é—´æ¡†æ¶éœ€è¦ timeframe_minutes/5 ä¸ª5åˆ†é’Ÿæ•°æ®
        periods_per_timeframe = timeframe_minutes // 5
        target_periods = 250  # ç›®æ ‡æ—¶é—´æ¡†æ¶å‘¨æœŸæ•°ï¼ˆæ¯”200å¤šä¸€äº›ä»¥ç¡®ä¿æ•°æ®å……è¶³ï¼‰
        periods_needed = periods_per_timeframe * target_periods
        
        logger.debug(f"Fetching {periods_needed} 5min periods for {timeframe_minutes}min analysis")
        
        cursor.execute("""
            SELECT timestamp, close_price, high_price, low_price, open_price, volume
            FROM crypto_5min_data 
            WHERE symbol = %s AND close_price > 0 AND high_price > 0 AND low_price > 0
            ORDER BY timestamp DESC
            LIMIT %s
        """, (symbol, periods_needed))
        
        raw_data = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if len(raw_data) < periods_per_timeframe * 50:  # è‡³å°‘éœ€è¦50ä¸ªç›®æ ‡å‘¨æœŸçš„æ•°æ®
            logger.warning(f"Insufficient 5min data for {symbol} {timeframe_minutes}m analysis: {len(raw_data)} records")
            return None
        
        logger.debug(f"Retrieved {len(raw_data)} 5min records for {symbol}")
        
        # å°†5åˆ†é’Ÿæ•°æ®èšåˆä¸ºç›®æ ‡æ—¶é—´æ¡†æ¶
        aggregated_data = aggregate_5min_to_timeframe(raw_data, timeframe_minutes)
        
        if len(aggregated_data) < 200:  # éœ€è¦è‡³å°‘200ä¸ªå‘¨æœŸæ¥è®¡ç®—SMA200
            logger.warning(f"Insufficient aggregated data for {symbol} {timeframe_minutes}m: {len(aggregated_data)} periods")
            return None
        
        logger.debug(f"Aggregated to {len(aggregated_data)} {timeframe_minutes}min periods")
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        indicators = calculate_technical_indicators(aggregated_data)
        
        if indicators is None:
            logger.warning(f"Failed to calculate indicators for {symbol} {timeframe_minutes}m")
            return None
        
        logger.debug(f"Successfully calculated indicators for {symbol} {timeframe_minutes}m")
        return indicators
        
    except Exception as e:
        logger.error(f"Error calculating indicators for {symbol} {timeframe_minutes}m: {str(e)}")
        return None

def aggregate_5min_to_timeframe(raw_data, timeframe_minutes):
    """å°†5åˆ†é’Ÿæ•°æ®èšåˆä¸ºæŒ‡å®šæ—¶é—´æ¡†æ¶çš„OHLCVæ•°æ®"""
    if not raw_data:
        return []
    
    # æŒ‰æ—¶é—´æ¡†æ¶åˆ†ç»„æ•°æ®
    periods_per_timeframe = timeframe_minutes // 5
    aggregated = []
    
    # åè½¬æ•°æ®ï¼Œä»æœ€æ—§åˆ°æœ€æ–°
    raw_data = list(reversed(raw_data))
    
    logger.debug(f"Aggregating {len(raw_data)} 5min periods to {timeframe_minutes}min timeframe")
    logger.debug(f"Periods per timeframe: {periods_per_timeframe}")
    
    for i in range(0, len(raw_data), periods_per_timeframe):
        period_data = raw_data[i:i + periods_per_timeframe]
        
        # å¯¹äºä¸å®Œæ•´çš„å‘¨æœŸï¼Œå¦‚æœæ•°æ®é‡è¶³å¤Ÿï¼ˆè‡³å°‘ä¸€åŠï¼‰ï¼Œä¹ŸåŒ…å«è¿›æ¥
        if len(period_data) < max(1, periods_per_timeframe // 2):
            continue
        
        try:
            # èšåˆOHLCVï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            timestamp = period_data[0][0]  # ä½¿ç”¨å‘¨æœŸå¼€å§‹æ—¶é—´
            open_price = float(period_data[0][4])  # ç¬¬ä¸€ä¸ª5åˆ†é’Ÿçš„å¼€ç›˜ä»·
            close_price = float(period_data[-1][1])  # æœ€åä¸€ä¸ª5åˆ†é’Ÿçš„æ”¶ç›˜ä»·
            
            # è¿‡æ»¤æ‰æ— æ•ˆä»·æ ¼æ•°æ®
            valid_highs = [float(row[2]) for row in period_data if row[2] is not None and float(row[2]) > 0]
            valid_lows = [float(row[3]) for row in period_data if row[3] is not None and float(row[3]) > 0]
            valid_volumes = [float(row[5]) for row in period_data if row[5] is not None and float(row[5]) >= 0]
            
            if not valid_highs or not valid_lows:
                continue  # è·³è¿‡æ— æ•ˆæ•°æ®
            
            high_price = max(valid_highs)  # å‘¨æœŸå†…æœ€é«˜ä»·
            low_price = min(valid_lows)  # å‘¨æœŸå†…æœ€ä½ä»·
            volume = sum(valid_volumes) if valid_volumes else 0  # å‘¨æœŸå†…æ€»æˆäº¤é‡
            
            # æ•°æ®éªŒè¯
            if high_price < low_price or open_price <= 0 or close_price <= 0:
                logger.warning(f"Invalid OHLC data: O={open_price}, H={high_price}, L={low_price}, C={close_price}")
                continue
            
            aggregated.append({
                'timestamp': timestamp,
                'open': open_price,
                'high': high_price,
                'low': low_price,
                'close': close_price,
                'volume': volume
            })
            
        except (ValueError, TypeError, IndexError) as e:
            logger.warning(f"Error aggregating period data: {str(e)}")
            continue
    
    logger.debug(f"Successfully aggregated to {len(aggregated)} {timeframe_minutes}min periods")
    return aggregated

def calculate_technical_indicators(ohlcv_data):
    """ä½¿ç”¨pandas-taåŸºäºOHLCVæ•°æ®è®¡ç®—ä¸“ä¸šæŠ€æœ¯æŒ‡æ ‡"""
    if len(ohlcv_data) < 200:
        logger.warning(f"Insufficient data for technical indicators: {len(ohlcv_data)} periods")
        return None
    
    try:
        # è½¬æ¢ä¸ºpandas DataFrame
        df = pd.DataFrame(ohlcv_data)
        df['open'] = pd.to_numeric(df['open'], errors='coerce')
        df['high'] = pd.to_numeric(df['high'], errors='coerce')
        df['low'] = pd.to_numeric(df['low'], errors='coerce')
        df['close'] = pd.to_numeric(df['close'], errors='coerce')
        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        
        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()
        
        if len(df) < 200:
            logger.warning(f"Insufficient clean data after removing NaN: {len(df)} periods")
            return None
        
        # è®¡ç®—SMA (Simple Moving Average)
        df['sma50'] = ta.sma(df['close'], length=50)
        df['sma200'] = ta.sma(df['close'], length=200)
        
        # è®¡ç®—ADXå’ŒDMIæŒ‡æ ‡
        adx_data = ta.adx(df['high'], df['low'], df['close'], length=14)
        df['adx'] = adx_data['ADX_14']
        df['plus_di'] = adx_data['DMP_14']
        df['minus_di'] = adx_data['DMN_14']
        
        # è®¡ç®—å¸ƒæ—å¸¦
        bbands = ta.bbands(df['close'], length=20, std=2)
        df['bb_upper'] = bbands['BBU_20_2.0']
        df['bb_middle'] = bbands['BBM_20_2.0']
        df['bb_lower'] = bbands['BBL_20_2.0']
        
        # è®¡ç®—å¸ƒæ—å¸¦å®½åº¦
        df['bandwidth'] = ((df['bb_upper'] - df['bb_lower']) / df['bb_middle'] * 100).fillna(0)
        
        # è®¡ç®—ATR (Average True Range)
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)
        
        # è·å–æœ€æ–°å€¼ï¼ˆå¤„ç†NaNï¼‰
        current_sma50 = df['sma50'].iloc[-1] if not pd.isna(df['sma50'].iloc[-1]) else df['close'].iloc[-1]
        current_sma200 = df['sma200'].iloc[-1] if not pd.isna(df['sma200'].iloc[-1]) else df['close'].iloc[-1]
        current_adx = df['adx'].iloc[-1] if not pd.isna(df['adx'].iloc[-1]) else 0
        current_plus_di = df['plus_di'].iloc[-1] if not pd.isna(df['plus_di'].iloc[-1]) else 0
        current_minus_di = df['minus_di'].iloc[-1] if not pd.isna(df['minus_di'].iloc[-1]) else 0
        current_atr = df['atr'].iloc[-1] if not pd.isna(df['atr'].iloc[-1]) else 0
        
        # è·å–æœ€è¿‘20ä¸ªæœ‰æ•ˆçš„å¸ƒæ—å¸¦å®½åº¦å€¼
        valid_bandwidths = df['bandwidth'].tail(20).dropna().tolist()
        if not valid_bandwidths:
            valid_bandwidths = [0]
        
        # è·å–æœ€è¿‘20ä¸ªæœ‰æ•ˆçš„ATRå€¼
        valid_atr_values = df['atr'].tail(20).dropna().tolist()
        if not valid_atr_values:
            valid_atr_values = [current_atr]
        
        logger.debug(f"Calculated indicators - SMA50: {current_sma50:.2f}, SMA200: {current_sma200:.2f}, "
                    f"ADX: {current_adx:.2f}, +DI: {current_plus_di:.2f}, -DI: {current_minus_di:.2f}")
        
        return {
            'price': df['close'].iloc[-1],
            'open': df['open'].iloc[-1],
            'high': df['high'].iloc[-1],
            'low': df['low'].iloc[-1],
            'close': df['close'].iloc[-1],
            'volume': df['volume'].iloc[-1],
            'sma50': current_sma50,
            'sma200': current_sma200,
            'adx': current_adx,
            'pdi': current_plus_di,
            'mdi': current_minus_di,
            'bandwidths': valid_bandwidths,
            'atr_values': valid_atr_values
        }
        
    except Exception as e:
        logger.error(f"Error calculating technical indicators with pandas-ta: {str(e)}")
        return None

def check_data_sufficiency(symbol):
    """æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿè¿›è¡Œåˆ†æ"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # æ£€æŸ¥æœ€è¿‘7å¤©çš„æ•°æ®é‡
        cursor.execute("""
            SELECT COUNT(*) FROM crypto_5min_data 
            WHERE symbol = %s AND timestamp >= DATE_SUB(NOW(), INTERVAL 7 DAY)
        """, (symbol,))
        
        recent_count = cursor.fetchone()[0]
        
        # æ£€æŸ¥æ€»æ•°æ®é‡
        cursor.execute("""
            SELECT COUNT(*) FROM crypto_5min_data WHERE symbol = %s
        """, (symbol,))
        
        total_count = cursor.fetchone()[0]
        
        cursor.close()
        conn.close()
        
        # è¯„ä¼°æ•°æ®å……è¶³æ€§
        sufficiency = {
            "total_records": total_count,
            "recent_records": recent_count,
            "can_analyze_15m": recent_count >= 60,    # éœ€è¦è‡³å°‘5å°æ—¶æ•°æ®
            "can_analyze_1h": recent_count >= 240,    # éœ€è¦è‡³å°‘20å°æ—¶æ•°æ®
            "can_analyze_4h": total_count >= 1000,    # éœ€è¦è‡³å°‘3-4å¤©æ•°æ®
            "can_analyze_1d": total_count >= 2000,    # éœ€è¦è‡³å°‘7å¤©æ•°æ®
            "can_analyze_1w": total_count >= 10000    # éœ€è¦è‡³å°‘35å¤©æ•°æ®
        }
        
        return sufficiency
        
    except Exception as e:
        logger.error(f"Error checking data sufficiency for {symbol}: {str(e)}")
        return None

def analyze_multiple_timeframes(symbol):
    """åŸºäºæ•°æ®åº“ä¸­çš„5åˆ†é’Ÿæ•°æ®åˆ†æå¤šä¸ªæ—¶é—´æ¡†æ¶çš„è¶‹åŠ¿"""
    timeframes = {
        "15m": {"minutes": 15, "name": "15åˆ†é’Ÿ"},
        "1h": {"minutes": 60, "name": "1å°æ—¶"},
        "4h": {"minutes": 240, "name": "4å°æ—¶"}, 
        "1d": {"minutes": 1440, "name": "1å¤©"},
        "1w": {"minutes": 10080, "name": "1å‘¨"}
    }
    
    # æ£€æŸ¥æ•°æ®å……è¶³æ€§
    data_sufficiency = check_data_sufficiency(symbol)
    if data_sufficiency:
        logger.info(f"{symbol} data status: {data_sufficiency['total_records']} total, {data_sufficiency['recent_records']} recent")
    
    trends = {}
    insights = {}
    
    for tf, config in timeframes.items():
        try:
            # æ£€æŸ¥è¯¥æ—¶é—´æ¡†æ¶æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®
            can_analyze_key = f"can_analyze_{tf}"
            if data_sufficiency and not data_sufficiency.get(can_analyze_key, False):
                logger.warning(f"Insufficient data for {symbol} {tf} analysis")
                trends[tf] = "æ•°æ®ç§¯ç´¯ä¸­"
                insights[tf] = f"[{config['name']}]æ•°æ®ç§¯ç´¯ä¸­ï¼Œè¯·ç­‰å¾…æ›´å¤šæ•°æ®æ”¶é›†åå†åˆ†æ"
                continue
            
            logger.info(f"Calculating {config['name']} indicators from 5min data for {symbol}")
            
            # åŸºäº5åˆ†é’Ÿæ•°æ®è®¡ç®—æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æŒ‡æ ‡
            indicators = calculate_indicators_from_5min_data(symbol, config['minutes'])
            
            if indicators is None:
                logger.warning(f"Could not calculate indicators for {symbol} {tf}")
                trends[tf] = "æ•°æ®ä¸è¶³"
                insights[tf] = f"[{config['name']}]æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿ã€‚å»ºè®®ç­‰å¾…æ›´å¤šæ•°æ®ç§¯ç´¯ã€‚"
                continue
            
            # åˆ¤æ–­è¶‹åŠ¿
            trend = determine_trend(indicators, tf)
            insight = generate_insight(symbol, trend, indicators, tf)
            
            trends[tf] = trend
            insights[tf] = insight
            
            # å­˜å‚¨è¶‹åŠ¿åˆ†æï¼ˆåŒ…å«ADXå¼ºåº¦ç”¨äºæœ‰æ•ˆæœŸè®¡ç®—ï¼‰
            adx_strength = indicators.get('adx', 0) if indicators else 0
            store_trend_analysis(symbol, tf, trend, insight, adx_strength)
            
            logger.info(f"{symbol} [{config['name']}] è¶‹åŠ¿: {trend}")
            
        except Exception as e:
            logger.error(f"Error analyzing {tf} timeframe for {symbol}: {str(e)}")
            trends[tf] = "é”™è¯¯"
            insights[tf] = f"åˆ†æ{config['name']}è¶‹åŠ¿æ—¶å‡ºé”™: {str(e)}"
    
    return trends, insights

def get_trend_validity_period(timeframe, adx_strength):
    """ä¼°ç®—è¶‹åŠ¿é¢„æµ‹çš„æœ‰æ•ˆæœŸ"""
    base_periods = {
        "15m": {"min": 1, "max": 4, "unit": "å°æ—¶"},
        "1h": {"min": 4, "max": 24, "unit": "å°æ—¶"},
        "4h": {"min": 1, "max": 7, "unit": "å¤©"},
        "1d": {"min": 1, "max": 4, "unit": "å‘¨"},
        "1w": {"min": 1, "max": 6, "unit": "ä¸ªæœˆ"}
    }
    
    if timeframe not in base_periods:
        return "æœªçŸ¥"
    
    period = base_periods[timeframe]
    
    # æ ¹æ®ADXå¼ºåº¦è°ƒæ•´æœ‰æ•ˆæœŸ
    if adx_strength > 30:  # å¼ºè¶‹åŠ¿
        validity = f"{period['max']}{period['unit']}"
        confidence = "é«˜"
    elif adx_strength > 20:  # ä¸­ç­‰è¶‹åŠ¿
        mid_period = (period['min'] + period['max']) // 2
        validity = f"{mid_period}{period['unit']}"
        confidence = "ä¸­ç­‰"
    else:  # å¼±è¶‹åŠ¿
        validity = f"{period['min']}{period['unit']}"
        confidence = "ä½"
    
    return f"é¢„æœŸæœ‰æ•ˆæœŸ{validity}(ç½®ä¿¡åº¦:{confidence})"

def generate_insight(symbol, trend, indicators=None, timeframe="5m"):
    timeframe_names = {
        "15m": "15åˆ†é’Ÿ", "1h": "1å°æ—¶", "4h": "4å°æ—¶", 
        "1d": "1å¤©", "1w": "1å‘¨"
    }
    
    tf_name = timeframe_names.get(timeframe, timeframe)
    
    # è·å–ADXå¼ºåº¦ç”¨äºæœ‰æ•ˆæœŸä¼°ç®—
    adx_strength = indicators.get('adx', 0) if indicators else 0
    validity_info = get_trend_validity_period(timeframe, adx_strength)
    
    # æ·»åŠ é£é™©æé†’
    risk_warning = ""
    if timeframe in ["15m", "1h"]:
        risk_warning = "âš ï¸çŸ­æœŸè¶‹åŠ¿æ˜“å—çªå‘äº‹ä»¶å½±å“"
    elif adx_strength < 20:
        risk_warning = "âš ï¸è¶‹åŠ¿å¼ºåº¦è¾ƒå¼±ï¼Œæ³¨æ„åè½¬é£é™©"
    
    base_insights = {
        "ä¸Šæ¶¨è¶‹åŠ¿": f"[{tf_name}]åŸºäºå½“å‰å¼ºåŠ¿ADXå’Œ+DIä¸»å¯¼ï¼Œé¢„è®¡ç»§ç»­ä¸Šæ¶¨è¶‹åŠ¿ï¼Œå¯èƒ½æµ‹è¯•æ›´é«˜é˜»åŠ›ä½ã€‚{validity_info}ã€‚{risk_warning}",
        "ä¸‹è·Œè¶‹åŠ¿": f"[{tf_name}]å½“å‰-DIä¸»å¯¼ä¸”SMAäº¤å‰å‘ä¸‹ï¼Œé¢„è®¡å»¶ç»­ä¸‹è·Œï¼Œå…³æ³¨æ”¯æ’‘ä½ã€‚{validity_info}ã€‚{risk_warning}",
        "åŒºé—´/æ³¢åŠ¨å°": f"[{tf_name}]ADXä½ä½ä¸”æ³¢åŠ¨ç‡ä½ï¼Œé¢„è®¡ç»´æŒéœ‡è¡ï¼Œç­‰å¾…çªç ´ä¿¡å·ã€‚{validity_info}ã€‚{risk_warning}"
    }
    
    if trend in base_insights:
        return base_insights[trend]
    else:
        # Enhanced insight for "æœªçŸ¥" trend
        if indicators:
            adx = indicators.get('adx', 0)
            pdi = indicators.get('pdi', 0)
            mdi = indicators.get('mdi', 0)
            sma50 = indicators.get('sma50', 0)
            sma200 = indicators.get('sma200', 0)
            
            details = []
            if adx <= 25:
                details.append(f"ADXè¾ƒä½({adx:.1f})è¡¨æ˜è¶‹åŠ¿å¼ºåº¦ä¸è¶³")
            if abs(pdi - mdi) < 5:
                details.append("ä¹°å–åŠ›é‡ç›¸å½“ï¼Œæ–¹å‘ä¸æ˜ç¡®")
            if abs(sma50 - sma200) / sma200 < 0.02:
                details.append("çŸ­é•¿æœŸå‡çº¿æ¥è¿‘ï¼Œç¼ºä¹æ˜ç¡®æ–¹å‘")
            
            if details:
                return f"[{tf_name}]å½“å‰ä¿¡å·æ··åˆï¼š{'; '.join(details)}ã€‚{validity_info}ã€‚å»ºè®®ç­‰å¾…æ›´æ˜ç¡®çš„çªç ´ä¿¡å·ã€‚"
        
        return f"[{tf_name}]è¶‹åŠ¿ä¿¡å·æ··åˆï¼Œå»ºè®®è§‚å¯Ÿå…³é”®æŠ€æœ¯ä½çªç ´æƒ…å†µã€‚{validity_info}ã€‚"

def store_5min_data(symbol, indicators, interval="5m"):
    """å­˜å‚¨5åˆ†é’ŸåŸå§‹æ•°æ®"""
    try:
        logger.debug(f"Storing 5min data for {symbol}")
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # Insert 5min data
        now = datetime.now()
        current_bw = indicators['bandwidths'][-1] if 'bandwidths' in indicators else None
        current_atr = indicators['atr_values'][-1] if 'atr_values' in indicators else None
        
        cursor.execute("""
            INSERT INTO crypto_5min_data 
            (symbol, timestamp, interval_type, price, open_price, high_price, low_price, close_price, volume, 
             adx, pdi, mdi, sma50, sma200, bandwidth, atr)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
            price=VALUES(price), open_price=VALUES(open_price), high_price=VALUES(high_price),
            low_price=VALUES(low_price), close_price=VALUES(close_price), volume=VALUES(volume),
            adx=VALUES(adx), pdi=VALUES(pdi), mdi=VALUES(mdi), sma50=VALUES(sma50), sma200=VALUES(sma200),
            bandwidth=VALUES(bandwidth), atr=VALUES(atr)
        """, (symbol, now, interval, 
              indicators.get('price', 0), indicators.get('open', 0), indicators.get('high', 0),
              indicators.get('low', 0), indicators.get('close', 0), indicators.get('volume', 0),
              indicators['adx'], indicators['pdi'], indicators['mdi'], 
              indicators['sma50'], indicators['sma200'], current_bw, current_atr))
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.debug(f"Successfully stored 5min data for {symbol}")
    except Exception as e:
        logger.error(f"MySQL error storing 5min data for {symbol}: {str(e)}")
        raise

def store_trend_analysis(symbol, timeframe, trend, insight, adx_strength=0):
    """å­˜å‚¨ä¸åŒæ—¶é—´æ¡†æ¶çš„è¶‹åŠ¿åˆ†æï¼ŒåŒ…å«æœ‰æ•ˆæœŸä¿¡æ¯"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # è®¡ç®—é¢„æœŸæœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        validity_hours = {
            "15m": 2 if adx_strength > 30 else 1,
            "1h": 12 if adx_strength > 30 else 6,
            "4h": 72 if adx_strength > 30 else 24,
            "1d": 336 if adx_strength > 30 else 168,  # 2å‘¨ vs 1å‘¨
            "1w": 2160 if adx_strength > 30 else 720   # 3ä¸ªæœˆ vs 1ä¸ªæœˆ
        }.get(timeframe, 24)
        
        now = datetime.now()
        cursor.execute("""
            INSERT INTO crypto_trends (symbol, timeframe, timestamp, trend, insight, adx_strength, expected_validity_hours)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
            trend=VALUES(trend), insight=VALUES(insight), adx_strength=VALUES(adx_strength), 
            expected_validity_hours=VALUES(expected_validity_hours), is_expired=FALSE
        """, (symbol, timeframe, now, trend, insight, adx_strength, validity_hours))
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.debug(f"Successfully stored trend analysis for {symbol} {timeframe} (validity: {validity_hours}h)")
    except Exception as e:
        logger.error(f"MySQL error storing trend for {symbol} {timeframe}: {str(e)}")
        raise

def check_expired_trends():
    """æ£€æŸ¥å¹¶æ ‡è®°è¿‡æœŸçš„è¶‹åŠ¿é¢„æµ‹"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # æ ‡è®°è¿‡æœŸçš„è¶‹åŠ¿
        cursor.execute("""
            UPDATE crypto_trends 
            SET is_expired = TRUE 
            WHERE is_expired = FALSE 
            AND TIMESTAMPDIFF(HOUR, timestamp, NOW()) > expected_validity_hours
        """)
        
        expired_count = cursor.rowcount
        
        conn.commit()
        cursor.close()
        conn.close()
        
        if expired_count > 0:
            logger.info(f"Marked {expired_count} trend predictions as expired")
            
    except Exception as e:
        logger.error(f"Error checking expired trends: {str(e)}")

def ensure_database_tables():
    """ç¡®ä¿æ•°æ®åº“è¡¨å­˜åœ¨"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # åˆ›å»º5åˆ†é’Ÿæ•°æ®è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS crypto_5min_data (
                id INT AUTO_INCREMENT PRIMARY KEY,
                symbol VARCHAR(20),
                timestamp DATETIME,
                interval_type VARCHAR(10),
                price FLOAT,
                open_price FLOAT,
                high_price FLOAT,
                low_price FLOAT,
                close_price FLOAT,
                volume FLOAT,
                adx FLOAT,
                pdi FLOAT,
                mdi FLOAT,
                sma50 FLOAT,
                sma200 FLOAT,
                bandwidth FLOAT,
                atr FLOAT,
                UNIQUE KEY unique_record (symbol, timestamp, interval_type)
            )
        """)
        
        # åˆ›å»ºè¶‹åŠ¿åˆ†æè¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS crypto_trends (
                id INT AUTO_INCREMENT PRIMARY KEY,
                symbol VARCHAR(20),
                timeframe VARCHAR(10),
                timestamp DATETIME,
                trend VARCHAR(50),
                insight TEXT,
                adx_strength FLOAT,
                expected_validity_hours INT,
                is_expired BOOLEAN DEFAULT FALSE,
                UNIQUE KEY unique_trend (symbol, timeframe, timestamp)
            )
        """)
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.info("Database tables created successfully")
        return True
        
    except Exception as e:
        logger.error(f"Error creating database tables: {str(e)}")
        return False

def initialize_historical_data(symbol):
    """åˆå§‹åŒ–æ—¶è·å–è¶³å¤Ÿçš„å†å²æ•°æ®"""
    try:
        logger.info(f"Initializing historical data for {symbol}...")
        
        # é¦–å…ˆç¡®ä¿è¡¨å­˜åœ¨
        if not ensure_database_tables():
            logger.error("Failed to create database tables")
            return False
        
        # æ£€æŸ¥ç°æœ‰æ•°æ®é‡
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT COUNT(*) FROM crypto_5min_data 
            WHERE symbol = %s AND timestamp >= DATE_SUB(NOW(), INTERVAL 7 DAY)
        """, (symbol,))
        
        existing_count = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        
        # å¦‚æœå·²æœ‰è¶³å¤Ÿæ•°æ®ï¼ˆ7å¤©çº¦2016ä¸ª5åˆ†é’Ÿæ•°æ®ç‚¹ï¼‰ï¼Œè·³è¿‡åˆå§‹åŒ–
        if existing_count >= 1500:
            logger.info(f"{symbol} already has sufficient data ({existing_count} records)")
            return True
        
        logger.info(f"{symbol} has {existing_count} records, need to fetch historical data")
        
        # è·å–ä¸åŒæ—¶é—´é—´éš”çš„å†å²æ•°æ®æ¥å¿«é€Ÿå¡«å……ï¼ˆå—APIé™åˆ¶ï¼Œæ¯æ¬¡æœ€å¤š20ä¸ªç»“æœï¼‰
        intervals_to_fetch = [
            ("5m", 20),    # æœ€è¿‘20ä¸ª5åˆ†é’Ÿæ•°æ®
            ("15m", 20),   # æœ€è¿‘20ä¸ª15åˆ†é’Ÿæ•°æ®  
            ("1h", 20),    # æœ€è¿‘20ä¸ª1å°æ—¶æ•°æ®
            ("4h", 20),    # æœ€è¿‘20ä¸ª4å°æ—¶æ•°æ®
            ("1d", 20),    # æœ€è¿‘20ä¸ª1å¤©æ•°æ®
        ]
        
        total_stored = 0
        
        for interval, batch_size in intervals_to_fetch:
            try:
                logger.info(f"Fetching {batch_size} {interval} historical data for {symbol}")
                
                # ä½¿ç”¨å¤šæ¬¡å°æ‰¹é‡è¯·æ±‚æ¥è·å–æ›´å¤šæ•°æ®
                batches_to_fetch = 3  # è·å–3æ‰¹ï¼Œæ€»å…±60ä¸ªæ•°æ®ç‚¹
                
                for batch_num in range(batches_to_fetch):
                    try:
                        logger.debug(f"Fetching batch {batch_num + 1}/{batches_to_fetch} for {interval}")
                        
                        # è·å–å†å²æ•°æ®
                        historical_data = fetch_historical_data_bulk(symbol, interval, batch_size)
                        
                        if historical_data:
                            # è½¬æ¢å¹¶å­˜å‚¨æ•°æ®
                            stored_count = store_historical_data_bulk(symbol, historical_data, interval, batch_num)
                            total_stored += stored_count
                            logger.debug(f"Stored {stored_count} {interval} records (batch {batch_num + 1})")
                        
                        # é¿å…APIé™åˆ¶ - æ‰¹æ¬¡é—´ç­‰å¾…æ›´é•¿æ—¶é—´
                        if batch_num < batches_to_fetch - 1:
                            time.sleep(10)
                            
                    except Exception as e:
                        logger.warning(f"Error fetching batch {batch_num + 1} of {interval} data: {str(e)}")
                        # å¦‚æœé‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                        if "rate-limit" in str(e).lower():
                            logger.info("Rate limit hit, waiting 30 seconds...")
                            time.sleep(30)
                        continue
                
                logger.info(f"Completed fetching {interval} data for {symbol}")
                
                # æ—¶é—´é—´éš”é—´ç­‰å¾…
                time.sleep(15)
                
            except Exception as e:
                logger.error(f"Error fetching {interval} data for {symbol}: {str(e)}")
                continue
        
        if total_stored == 0:
            logger.warning(f"No historical data could be fetched for {symbol} due to API limits")
            logger.info(f"System will start with current data and accumulate over time")
        
        logger.info(f"Historical data initialization completed for {symbol}: {total_stored} total records")
        return total_stored > 0
        
    except Exception as e:
        logger.error(f"Error initializing historical data for {symbol}: {str(e)}")
        return False

def fetch_historical_data_bulk(symbol, interval, limit):
    """æ‰¹é‡è·å–å†å²æ•°æ®"""
    url = "https://api.taapi.io/bulk"
    payload = {
        "secret": TAAPI_KEY,
        "construct": {
            "exchange": "binance",
            "symbol": symbol,
            "interval": interval,
            "indicators": [
                {"indicator": "price", "results": limit, "id": "price_history"}
            ]
        }
    }
    
    try:
        response = requests.post(url, json=payload, timeout=60)
        if response.status_code == 200:
            data = response.json()['data']
            for item in data:
                if item['id'] == 'price_history':
                    return item['result']
        else:
            logger.error(f"API error fetching {interval} history for {symbol}: {response.text}")
            return None
    except Exception as e:
        logger.error(f"Request error fetching {interval} history for {symbol}: {str(e)}")
        return None

def store_historical_data_bulk(symbol, historical_data, interval, batch_offset=0):
    """æ‰¹é‡å­˜å‚¨å†å²æ•°æ®"""
    if not historical_data:
        return 0
    
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        stored_count = 0
        
        # å°†ä¸åŒæ—¶é—´é—´éš”çš„æ•°æ®è½¬æ¢ä¸º5åˆ†é’Ÿæ•°æ®ç‚¹
        for record in historical_data:
            try:
                # è§£ææ—¶é—´æˆ³
                if 'timestamp' in record:
                    timestamp = datetime.fromtimestamp(record['timestamp'])
                else:
                    continue
                
                # æ ¹æ®æ—¶é—´é—´éš”åˆ›å»ºå¤šä¸ª5åˆ†é’Ÿæ•°æ®ç‚¹
                interval_minutes = {
                    "5m": 5, "15m": 15, "1h": 60, "4h": 240, "1d": 1440
                }.get(interval, 5)
                
                # ä¸ºäº†å¿«é€Ÿå¡«å……æ•°æ®ï¼Œæˆ‘ä»¬å°†è¾ƒå¤§æ—¶é—´é—´éš”çš„æ•°æ®åˆ†è§£ä¸ºå¤šä¸ª5åˆ†é’Ÿç‚¹
                points_per_interval = max(1, interval_minutes // 5)
                
                # æ·»åŠ æ‰¹æ¬¡åç§»ï¼Œé¿å…é‡å¤æ—¶é—´æˆ³
                base_offset_minutes = batch_offset * interval_minutes
                
                for i in range(points_per_interval):
                    point_timestamp = timestamp - pd.Timedelta(minutes=base_offset_minutes + i*5)
                    
                    cursor.execute("""
                        INSERT IGNORE INTO crypto_5min_data 
                        (symbol, timestamp, interval_type, price, open_price, high_price, low_price, close_price, volume, 
                         adx, pdi, mdi, sma50, sma200, bandwidth, atr)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        symbol, point_timestamp, "5m",
                        record.get('value', 0),
                        record.get('open', record.get('value', 0)),
                        record.get('high', record.get('value', 0)),
                        record.get('low', record.get('value', 0)),
                        record.get('value', 0),  # close
                        record.get('volume', 0),
                        0, 0, 0, 0, 0, 0, 0  # æŠ€æœ¯æŒ‡æ ‡ç¨åè®¡ç®—
                    ))
                    
                    if cursor.rowcount > 0:
                        stored_count += 1
                        
            except Exception as e:
                logger.warning(f"Error storing historical record: {str(e)}")
                continue
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return stored_count
        
    except Exception as e:
        logger.error(f"Error storing historical data bulk: {str(e)}")
        return 0

def cleanup_old_data():
    """æ¸…ç†90å¤©å‰çš„æ•°æ®"""
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB
        )
        cursor = conn.cursor()
        
        # æ¸…ç†5åˆ†é’Ÿæ•°æ®ï¼ˆä¿ç•™90å¤©ï¼‰
        cursor.execute("""
            DELETE FROM crypto_5min_data 
            WHERE timestamp < DATE_SUB(NOW(), INTERVAL 90 DAY)
        """)
        deleted_5min = cursor.rowcount
        
        # æ¸…ç†è¶‹åŠ¿æ•°æ®ï¼ˆä¿ç•™90å¤©ï¼‰
        cursor.execute("""
            DELETE FROM crypto_trends 
            WHERE timestamp < DATE_SUB(NOW(), INTERVAL 90 DAY)
        """)
        deleted_trends = cursor.rowcount
        
        conn.commit()
        cursor.close()
        conn.close()
        
        if deleted_5min > 0 or deleted_trends > 0:
            logger.info(f"Cleaned up old data: {deleted_5min} 5min records, {deleted_trends} trend records")
        
    except Exception as e:
        logger.error(f"Error cleaning up old data: {str(e)}")

def send_to_telegram(message):
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage?chat_id={TELEGRAM_CHAT_ID}&text={message}"
        response = requests.get(url)
        if response.status_code != 200:
            logger.error(f"Telegram send error: {response.text}")
        else:
            logger.info("Telegram message sent successfully")
    except Exception as e:
        logger.error(f"Failed to send Telegram message: {str(e)}")

if __name__ == "__main__":
    symbols = ["BTC/USDT", "ETH/USDT"]
    last_trends = {sym: {tf: None for tf in ["15m", "1h", "4h", "1d", "1w"]} for sym in symbols}
    
    logger.info("Multi-timeframe crypto trend bot started successfully...")
    logger.info("Data strategy: Collect 5min data from API, calculate 15m/1h/4h/1d/1w trends from database")
    logger.info("Monitoring timeframes: 15m, 1h, 4h, 1d, 1w")
    logger.info("Data retention: 90 days, cleanup daily at midnight")
    
    # é¦–å…ˆç¡®ä¿æ•°æ®åº“è¡¨å­˜åœ¨
    logger.info("Setting up database tables...")
    if not ensure_database_tables():
        logger.error("Failed to create database tables. Exiting...")
        sys.exit(1)
    
    # åˆå§‹åŒ–å†å²æ•°æ®
    logger.info("Checking and initializing historical data...")
    logger.info("Note: Due to API rate limits, initialization may take several minutes")
    
    initialization_results = {}
    
    for symbol in symbols:
        try:
            logger.info(f"Starting initialization for {symbol}...")
            success = initialize_historical_data(symbol)
            initialization_results[symbol] = success
            
            if success:
                logger.info(f"âœ“ Successfully initialized historical data for {symbol}")
            else:
                logger.warning(f"âš  Limited initialization for {symbol} - will accumulate data over time")
            
            # ç¬¦å·é—´ç­‰å¾…æ›´é•¿æ—¶é—´é¿å…APIé™åˆ¶
            if symbol != symbols[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªç¬¦å·
                logger.info("Waiting 30 seconds before next symbol to avoid API limits...")
                time.sleep(30)
                
        except Exception as e:
            logger.error(f"Error during initialization for {symbol}: {str(e)}")
            initialization_results[symbol] = False
    
    successful_inits = sum(1 for success in initialization_results.values() if success)
    total_symbols = len(symbols)
    
    if successful_inits == total_symbols:
        logger.info("âœ“ Historical data initialization completed successfully for all symbols")
        logger.info("All timeframes should be available for analysis")
    elif successful_inits > 0:
        logger.info(f"âš  Partial initialization success ({successful_inits}/{total_symbols} symbols)")
        logger.info("System will start with mixed analysis capabilities")
    else:
        logger.warning("âš  Historical data initialization failed for all symbols")
        logger.info("System will start with minimal capabilities and accumulate data over time")
        logger.info("This is normal when API rate limits are strict - analysis will improve over time")
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®çŠ¶æ€
    for symbol in symbols:
        try:
            sufficiency = check_data_sufficiency(symbol)
            if sufficiency:
                available_timeframes = []
                for tf in ["15m", "1h", "4h", "1d", "1w"]:
                    if sufficiency.get(f"can_analyze_{tf}", False):
                        available_timeframes.append(tf)
                
                if available_timeframes:
                    logger.info(f"{symbol} - Available timeframes: {', '.join(available_timeframes)}")
                else:
                    logger.info(f"{symbol} - No timeframes ready yet, will accumulate data")
        except Exception as e:
            logger.error(f"Error checking initial data status for {symbol}: {str(e)}")
    
    logger.info("Starting main monitoring loop...")
    
    while True:
        try:
            logger.info("Starting new multi-timeframe analysis cycle...")
            all_trends = {}
            all_insights = {}
            trend_changed = False
            
            # æ¯å¤©æ¸…ç†ä¸€æ¬¡æ—§æ•°æ®å’Œæ£€æŸ¥è¿‡æœŸè¶‹åŠ¿
            current_hour = datetime.now().hour
            if current_hour == 0:  # æ¯å¤©åˆå¤œæ¸…ç†
                logger.info("Performing daily data cleanup...")
                cleanup_old_data()
                check_expired_trends()
            
            for i, symbol in enumerate(symbols):
                logger.info(f"Analyzing {symbol} across multiple timeframes...")
                
                # Add delay between symbols to avoid rate limiting
                if i > 0:
                    logger.info("Waiting 10 seconds to avoid rate limit...")
                    time.sleep(10)
                
                # é¦–å…ˆè·å–5åˆ†é’Ÿæ•°æ®å¹¶å­˜å‚¨ï¼ˆè¿™æ˜¯å”¯ä¸€çš„APIè°ƒç”¨ï¼‰
                try:
                    data_5m = fetch_taapi_data(symbol, "5m")
                    indicators_5m = parse_indicators(data_5m)
                    store_5min_data(symbol, indicators_5m, "5m")
                    logger.info(f"Stored 5min data for {symbol}")
                except Exception as e:
                    logger.error(f"Failed to store 5min data for {symbol}: {str(e)}")
                    continue  # å¦‚æœæ— æ³•è·å–5åˆ†é’Ÿæ•°æ®ï¼Œè·³è¿‡è¿™ä¸ªsymbol
                
                # åŸºäºæ•°æ®åº“ä¸­çš„5åˆ†é’Ÿæ•°æ®åˆ†æå¤šä¸ªæ—¶é—´æ¡†æ¶
                symbol_trends, symbol_insights = analyze_multiple_timeframes(symbol)
                all_trends[symbol] = symbol_trends
                all_insights[symbol] = symbol_insights
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶‹åŠ¿å˜åŒ–
                for timeframe in ["15m", "1h", "4h", "1d", "1w"]:
                    current_trend = symbol_trends.get(timeframe, "æœªçŸ¥")
                    if current_trend != last_trends[symbol][timeframe]:
                        trend_changed = True
                        logger.info(f"{symbol} [{timeframe}] è¶‹åŠ¿å˜åŒ–: {last_trends[symbol][timeframe]} -> {current_trend}")
                        last_trends[symbol][timeframe] = current_trend
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                logger.info("Waiting 15 seconds before next symbol...")
                time.sleep(15)
            
            # å‘é€é€šçŸ¥ï¼ˆå¦‚æœæœ‰è¶‹åŠ¿å˜åŒ–ï¼‰
            if trend_changed:
                message = "ğŸ”„ åŠ å¯†è´§å¸å¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿æ›´æ–°\n\n"
                
                for symbol in symbols:
                    coin_name = symbol.split('/')[0]
                    message += f"ğŸ’° {coin_name}:\n"
                    
                    trends = all_trends[symbol]
                    insights = all_insights[symbol]
                    
                    # åªæ˜¾ç¤ºä¸»è¦æ—¶é—´æ¡†æ¶
                    main_timeframes = ["15m", "1h", "4h", "1d", "1w"]
                    for tf in main_timeframes:
                        if tf in trends:
                            tf_name = {"15m": "15åˆ†é’Ÿ", "1h": "1å°æ—¶", "4h": "4å°æ—¶", "1d": "1å¤©", "1w": "1å‘¨"}[tf]
                            message += f"  {tf_name}: {trends[tf]}\n"
                    
                    # æ·»åŠ 1å¤©çš„è¯¦ç»†åˆ†æ
                    if "1d" in insights:
                        message += f"  ğŸ“Š {insights['1d']}\n"
                    
                    message += "\n"
                
                logger.info("Multi-timeframe trend changes detected, sending notification...")
                send_to_telegram(message.strip())
                logger.info("Multi-timeframe notification sent")
            else:
                logger.info("No trend changes detected across all timeframes")
                
            logger.info("Multi-timeframe analysis cycle completed, waiting 5 minut..")
            
        except Exception as e:
            logger.error(f"Error in main loop: {str(e)}")
            logger.info("Continuing after error...")
            
        time.sleep(300)  # æ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡