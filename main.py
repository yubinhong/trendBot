import os
import sys
import time
import logging
from datetime import datetime
from typing import Dict, List, Any

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from database import DatabaseManager
from api_client import BinanceAPIClient
from technical_analysis import TechnicalAnalyzer
from quality_monitor import DataQualityMonitor
from notification import NotificationManager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Force stdout to be unbuffered
sys.stdout.reconfigure(line_buffering=True)

# Load environment variables for security
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
MYSQL_HOST = os.getenv('MYSQL_HOST')
MYSQL_USER = os.getenv('MYSQL_USER')
MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD')
MYSQL_DB = os.getenv('MYSQL_DB')
SKIP_INITIALIZATION = os.getenv('SKIP_INITIALIZATION', 'false').lower() == 'true'

# Data granularity configuration
DATA_GRANULARITY = os.getenv('DATA_GRANULARITY', '1m')  # '1m' or '5m'
SMART_GRANULARITY = os.getenv('SMART_GRANULARITY', 'true').lower() == 'true'
VOLATILITY_THRESHOLD = float(os.getenv('VOLATILITY_THRESHOLD', '2.0'))  # ATR multiplier for high volatility
API_RATE_LIMIT_BUFFER = float(os.getenv('API_RATE_LIMIT_BUFFER', '0.8'))  # Use 80% of API limit

if not all([TELEGRAM_TOKEN, TELEGRAM_CHAT_ID, MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DB]):
    logger.error("Missing required environment variables.")
    raise ValueError("Missing required environment variables.")

logger.info("Starting crypto trend bot...")
logger.info(f"Monitoring symbols: BTC/USDT, ETH/USDT")
logger.info(f"MySQL Host: {MYSQL_HOST}")
logger.info(f"Telegram Chat ID: {TELEGRAM_CHAT_ID}")
logger.info(f"Data Granularity: {DATA_GRANULARITY}")
logger.info(f"Smart Granularity: {SMART_GRANULARITY}")
logger.info(f"Volatility Threshold: {VOLATILITY_THRESHOLD}")
logger.info(f"API Rate Limit Buffer: {API_RATE_LIMIT_BUFFER}")

# åˆå§‹åŒ–ç»„ä»¶
db_manager = DatabaseManager(MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DB)
api_client = BinanceAPIClient(API_RATE_LIMIT_BUFFER)
technical_analyzer = TechnicalAnalyzer(db_manager, VOLATILITY_THRESHOLD)
quality_monitor = DataQualityMonitor(db_manager, api_client)
notification_manager = NotificationManager()

# Test database connection
try:
    conn = db_manager.get_connection()
    conn.close()
    logger.info("âœ“ Database connection successful")
except Exception as e:
    logger.error(f"âœ— Database connection failed: {str(e)}")
    logger.error("Please check your database configuration and ensure MySQL is running")
    sys.exit(1)

# ç¡®ä¿æ•°æ®åº“è¡¨å­˜åœ¨
db_manager.ensure_database_tables()

# ç›‘æ§çš„å¸ç§
symbols = ['BTCUSDT', 'ETHUSDT']

# å­˜å‚¨ä¸Šæ¬¡çš„è¶‹åŠ¿çŠ¶æ€
last_trends = {symbol: {} for symbol in symbols}

def send_individual_trend_notification(symbol: str, trends: Dict[str, str], insights: Dict[str, Any], changed_timeframes: List[tuple]):
    """ä¸ºå•ä¸ªå¸ç§å‘é€è¶‹åŠ¿å˜åŒ–é€šçŸ¥"""
    try:
        coin_name = symbol.replace('USDT', '')
        
        # ç”Ÿæˆé’ˆå¯¹å•ä¸ªå¸ç§çš„é€šçŸ¥æ¶ˆæ¯
        message = f"ğŸš¨ {coin_name} è¶‹åŠ¿å˜åŒ–æé†’\n"
        message += f"â° {datetime.now().strftime('%H:%M')}\n\n"
        
        # æ£€æŸ¥æ•°æ®çŠ¶æ€
        data_status = check_data_sufficiency(symbol)
        total_records = data_status.get('total_records', 0)
        days_available = total_records / 1440 if total_records > 0 else 0  # æ¯å¤©1440ä¸ª1åˆ†é’Ÿæ•°æ®ç‚¹
        
        message += f"ğŸ’ {coin_name} è¶‹åŠ¿åˆ†æ\n"
        message += f"ğŸ“ˆ æ•°æ®: {days_available:.1f}å¤© ({total_records}æ¡)\n\n"
        
        # è¶‹åŠ¿çŠ¶æ€ï¼ˆä½¿ç”¨emojiè¡¨ç¤ºï¼‰
        trend_emojis = {
            "ä¸Šæ¶¨è¶‹åŠ¿": "ğŸŸ¢",
            "ä¸‹è·Œè¶‹åŠ¿": "ğŸ”´", 
            "åŒºé—´/æ³¢åŠ¨å°": "ğŸŸ¡",
            "æœªçŸ¥": "âšª",
            "æ•°æ®ä¸è¶³": "â³",
            "æ•°æ®ç§¯ç´¯ä¸­": "â³"
        }
        
        # æ˜¾ç¤ºå˜åŒ–çš„æ—¶é—´æ¡†æ¶
        message += "ğŸ“Š è¶‹åŠ¿å˜åŒ–:\n"
        for timeframe, old_trend, new_trend in changed_timeframes:
            tf_name = {"1m": "1åˆ†é’Ÿ", "5m": "5åˆ†é’Ÿ"}[timeframe]  # åªä¿ç•™1åˆ†é’Ÿå’Œ5åˆ†é’Ÿ
            old_emoji = trend_emojis.get(old_trend, "â“")
            new_emoji = trend_emojis.get(new_trend, "â“")
            
            # è·å–è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯
            strength_info = ""
            if timeframe in insights and insights[timeframe]:
                insight_text = insights[timeframe]
                if "å¼ºè¶‹åŠ¿" in insight_text:
                    strength_info = "ğŸ’ª"
                elif "ä¸­è¶‹åŠ¿" in insight_text:
                    strength_info = "ğŸ“ˆ"
                elif "å¼±è¶‹åŠ¿" in insight_text:
                    strength_info = "ğŸ“‰"
            
            message += f"  {tf_name}: {old_emoji}{old_trend} â†’ {new_emoji}{new_trend}{strength_info}\n"
        
        message += "\nğŸ“ˆ å½“å‰æ‰€æœ‰æ—¶é—´æ¡†æ¶:\n"
        main_timeframes = ["1m", "5m"]  # åªä¿ç•™1åˆ†é’Ÿå’Œ5åˆ†é’Ÿ
        for tf in main_timeframes:
            if tf in trends:
                tf_name = {"1m": "1åˆ†é’Ÿ", "5m": "5åˆ†é’Ÿ"}[tf]
                trend = trends[tf]
                emoji = trend_emojis.get(trend, "â“")
                
                # è·å–è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯
                strength_info = ""
                if tf in insights and insights[tf]:
                    insight_text = insights[tf]
                    if "å¼ºè¶‹åŠ¿" in insight_text:
                        strength_info = "ğŸ’ª"
                    elif "ä¸­è¶‹åŠ¿" in insight_text:
                        strength_info = "ğŸ“ˆ"
                    elif "å¼±è¶‹åŠ¿" in insight_text:
                        strength_info = "ğŸ“‰"
                
                # æ·»åŠ åˆ†æè´¨é‡æ ‡è¯†
                quality_indicator = ""
                if tf == "4h" and days_available < 30:
                    quality_indicator = " (åŸºç¡€)"
                elif tf == "1d" and days_available < 200:
                    quality_indicator = " (åŸºç¡€)"
                
                message += f"  {emoji} {tf_name}: {trend}{strength_info}{quality_indicator}\n"
        
        # æ·»åŠ æ•°æ®è´¨é‡è¯´æ˜
        message += "\nâ„¹ï¸ è¯´æ˜:\n"
        message += "ğŸŸ¢ä¸Šæ¶¨ ğŸ”´ä¸‹è·Œ ğŸŸ¡éœ‡è¡ âšªæ··åˆ â³ç§¯ç´¯ä¸­\n"
        message += "ğŸ’ªå¼ºè¶‹åŠ¿ ğŸ“ˆä¸­ç­‰è¶‹åŠ¿ ğŸ“‰å¼±è¶‹åŠ¿\n"
        message += "(åŸºç¡€)=æ•°æ®ç§¯ç´¯ä¸­ï¼Œåˆ†æä¼šæŒç»­æ”¹å–„\n"
        message += f"ğŸ“Š æ¯5åˆ†é’Ÿæ›´æ–° | æ•°æ®ä¿ç•™250å¤©"
        
        logger.info(f"Sending individual trend notification for {symbol}...")
        notification_manager.send_telegram_message(message.strip())
        logger.info(f"Individual trend notification sent for {symbol}")
        
    except Exception as e:
        logger.error(f"Error sending individual notification for {symbol}: {str(e)}")

def check_data_sufficiency(symbol: str) -> Dict[str, Any]:
    """æ£€æŸ¥æ•°æ®å……è¶³æ€§"""
    try:
        conn = db_manager.get_connection()
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ€»æ•°æ®é‡ï¼ˆç”¨äºåˆå§‹åŒ–åˆ¤æ–­ï¼‰
        cursor.execute("""
            SELECT COUNT(*) FROM crypto_1min_data 
            WHERE symbol = %s
        """, (symbol,))
        
        total_records = cursor.fetchone()[0]
        
        # æ£€æŸ¥æœ€è¿‘30å¤©çš„æ•°æ®é‡ï¼ˆç”¨äºåˆ†æå¯è¡Œæ€§åˆ¤æ–­ï¼‰
        cursor.execute("""
            SELECT COUNT(*) FROM crypto_1min_data 
            WHERE symbol = %s AND timestamp >= DATE_SUB(NOW(), INTERVAL 30 DAY)
        """, (symbol,))
        
        recent_30d_records = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        
        # è®¡ç®—å¯ç”¨å¤©æ•°ï¼ˆæ¯å¤©1440ä¸ª1åˆ†é’Ÿæ•°æ®ç‚¹ï¼‰
        days_available = recent_30d_records / 1440 if recent_30d_records > 0 else 0
        total_days_available = total_records / 1440 if total_records > 0 else 0
        
        return {
            'total_records': total_records,
            'recent_30d_records': recent_30d_records,
            'days_available': days_available,
            'total_days_available': total_days_available,
            # åˆ†æå¯è¡Œæ€§åŸºäºæ€»æ•°æ®é‡ï¼Œå› ä¸ºæŠ€æœ¯æŒ‡æ ‡éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
            # 1åˆ†é’Ÿï¼šéœ€è¦è‡³å°‘200ä¸ªæ•°æ®ç‚¹ï¼ˆçº¦3.3å°æ—¶ï¼‰
            'can_analyze_1m': total_records >= 200,
            # 5åˆ†é’Ÿï¼šéœ€è¦è‡³å°‘1000ä¸ª1åˆ†é’Ÿæ•°æ®ç‚¹ï¼ˆçº¦17å°æ—¶ï¼‰
            'can_analyze_5m': total_records >= 1000
        }
        
    except Exception as e:
        logger.error(f"Error checking data sufficiency for {symbol}: {str(e)}")
        return {
            'total_records': 0,
            'recent_30d_records': 0,
            'days_available': 0,
            'total_days_available': 0,
            'can_analyze_5m': False,
            'can_analyze_15m': False,
            'can_analyze_1h': False,
            'can_analyze_4h': False,
            'can_analyze_1d': False
        }

def analyze_multiple_timeframes(symbol: str) -> tuple[Dict[str, str], Dict[str, str]]:
    """åˆ†æå¤šä¸ªæ—¶é—´æ¡†æ¶ - åªä¿ç•™1åˆ†é’Ÿå’Œ5åˆ†é’Ÿ"""
    trends = {}
    insights = {}
    
    # æ£€æŸ¥æ•°æ®å……è¶³æ€§
    data_status = check_data_sufficiency(symbol)
    
    # åªä¿ç•™1åˆ†é’Ÿå’Œ5åˆ†é’Ÿæ—¶é—´æ¡†æ¶
    timeframes = {
        "1m": 1,
        "5m": 5
    }
    
    for tf_name, tf_minutes in timeframes.items():
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œåˆ†æ
            can_analyze_key = f"can_analyze_{tf_name}"
            if not data_status.get(can_analyze_key, False):
                trends[tf_name] = "æ•°æ®ç§¯ç´¯ä¸­"
                insights[tf_name] = f"[éœ€è¦æ›´å¤šæ•°æ®] æ€»è®¡: {data_status['total_records']}æ¡è®°å½• ({data_status['total_days_available']:.1f}å¤©)"
                continue
            
            # ä½¿ç”¨æŠ€æœ¯åˆ†æå™¨åˆ†æè¶‹åŠ¿
            trend_result = technical_analyzer.analyze_trend(symbol, tf_minutes)
            
            if trend_result and trend_result.get('direction') != 'æœªçŸ¥':
                trend = trend_result['direction']
                trends[tf_name] = trend
                
                # ç”Ÿæˆæ´å¯Ÿ
                confidence = trend_result.get('confidence', 0)
                insights_text = trend_result.get('insights', '')
                
                if confidence > 70:
                    insights[tf_name] = f"é«˜ç½®ä¿¡åº¦({confidence:.1f}%)ï¼Œ{trend}ä¿¡å·å¼ºçƒˆ"
                elif confidence > 50:
                    insights[tf_name] = f"ä¸­ç­‰ç½®ä¿¡åº¦({confidence:.1f}%)ï¼Œ{trend}ä¿¡å·è¾ƒæ˜ç¡®"
                else:
                    insights[tf_name] = f"ä½ç½®ä¿¡åº¦({confidence:.1f}%)ï¼Œ{trend}ä¿¡å·éœ€è¦ç¡®è®¤"
                    
                if insights_text:
                    insights[tf_name] += f" - {insights_text}"
            else:
                trends[tf_name] = "æ•°æ®ä¸è¶³"
                insights[tf_name] = "æ— æ³•è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"
                
        except Exception as e:
            logger.error(f"Error analyzing {tf_name} timeframe for {symbol}: {str(e)}")
            trends[tf_name] = "åˆ†æé”™è¯¯"
            insights[tf_name] = f"åˆ†æå‡ºé”™: {str(e)[:50]}"
    
    return trends, insights

def initialize_historical_data(symbol: str, limit: int = 2000) -> bool:
    """åˆå§‹åŒ–å†å²æ•°æ® - åˆ†æ‰¹è·å–2000æ¡æ•°æ®"""
    try:
        logger.info(f"Initializing historical data for {symbol} (limit: {limit})...")
        
        # è®¡ç®—éœ€è¦çš„æ‰¹æ¬¡æ•°ï¼ˆæ¯æ¬¡æœ€å¤š1000æ¡ï¼‰
        batch_size = 1000
        total_batches = (limit + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
        
        logger.info(f"Will fetch data in {total_batches} batches of {batch_size} records each")
        
        all_klines = []
        end_time = None  # ä»æœ€æ–°æ•°æ®å¼€å§‹è·å–
        
        for batch_num in range(total_batches):
            try:
                logger.info(f"Fetching batch {batch_num + 1}/{total_batches} for {symbol}...")
                
                # è·å–å½“å‰æ‰¹æ¬¡çš„Kçº¿æ•°æ®
                if end_time is None:
                    # ç¬¬ä¸€æ‰¹ï¼šè·å–æœ€æ–°çš„1000æ¡æ•°æ®
                    klines = api_client.fetch_binance_klines(symbol, '1m', batch_size)
                else:
                    # åç»­æ‰¹æ¬¡ï¼šä½¿ç”¨endTimeå‚æ•°è·å–æ›´æ—©çš„æ•°æ®
                    klines = api_client.fetch_binance_klines_with_endtime(symbol, '1m', batch_size, end_time)
                
                if not klines:
                    logger.warning(f"No data returned for batch {batch_num + 1}, stopping...")
                    break
                
                logger.info(f"Retrieved {len(klines)} klines for batch {batch_num + 1}")
                
                # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
                all_klines.extend(klines)
                
                # è®¾ç½®ä¸‹ä¸€æ‰¹æ¬¡çš„ç»“æŸæ—¶é—´ï¼ˆå½“å‰æ‰¹æ¬¡æœ€æ—©çš„æ—¶é—´æˆ³ï¼‰
                if klines:
                    # è·å–å½“å‰æ‰¹æ¬¡æœ€æ—©çš„æ—¶é—´æˆ³ï¼Œå‡1æ¯«ç§’ä½œä¸ºä¸‹ä¸€æ‰¹æ¬¡çš„endTime
                    # timestampæ˜¯datetimeå¯¹è±¡ï¼Œéœ€è¦è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
                    earliest_timestamp = min(int(k['timestamp'].timestamp() * 1000) for k in klines)
                    end_time = earliest_timestamp - 1
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                if batch_num < total_batches - 1:  # ä¸æ˜¯æœ€åä¸€æ‰¹
                    logger.info("Waiting 2 seconds before next batch...")
                    time.sleep(2)
                    
            except Exception as e:
                logger.error(f"Error fetching batch {batch_num + 1}: {str(e)}")
                # ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡ï¼Œä¸ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
                continue
        
        if not all_klines:
            logger.error(f"Failed to fetch any historical klines for {symbol}")
            return False
        
        logger.info(f"Retrieved total {len(all_klines)} historical klines for {symbol}")
        
        # æ‰¹é‡å­˜å‚¨å†å²æ•°æ®
        success = db_manager.store_historical_klines_bulk(symbol, all_klines, '1m')
        
        if success:
            logger.info(f"Successfully stored {len(all_klines)} historical records for {symbol}")
            return True
        else:
            logger.error(f"Failed to store historical data for {symbol}")
            return False
            
    except Exception as e:
        logger.error(f"Error initializing historical data for {symbol}: {str(e)}")
        return False

def cleanup_old_data():
    """æ¸…ç†æ—§æ•°æ®"""
    try:
        db_manager.cleanup_old_data()
        logger.info("Old data cleanup completed")
    except Exception as e:
        logger.error(f"Error during data cleanup: {str(e)}")

def check_expired_trends():
    """æ£€æŸ¥è¿‡æœŸè¶‹åŠ¿"""
    try:
        conn = db_manager.get_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            DELETE FROM crypto_trends 
            WHERE timestamp < DATE_SUB(NOW(), INTERVAL 7 DAY)
        """)
        
        deleted_count = cursor.rowcount
        conn.commit()
        cursor.close()
        conn.close()
        
        if deleted_count > 0:
            logger.info(f"Cleaned up {deleted_count} expired trend records")
            
    except Exception as e:
        logger.error(f"Error cleaning expired trends: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== Crypto Trend Analysis Bot Started ===")
    
    # åˆå§‹åŒ–å†å²æ•°æ®ï¼ˆå¦‚æœæœªè·³è¿‡ï¼‰
    if not SKIP_INITIALIZATION:
        logger.info("Checking existing data before initialization...")
        initialization_results = {}
        
        for symbol in symbols:
            try:
                # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰è¶³å¤Ÿæ•°æ®
                data_status = check_data_sufficiency(symbol)
                total_records = data_status.get('total_records', 0)
                
                if total_records >= 2000:  # å¦‚æœå·²æœ‰2000æ¡ä»¥ä¸Šæ•°æ®ï¼Œè·³è¿‡åˆå§‹åŒ–
                    logger.info(f"âœ“ {symbol} already has sufficient data ({total_records} records), skipping initialization")
                    initialization_results[symbol] = True
                    continue
                
                logger.info(f"Initializing data for {symbol} (current: {total_records} records)...")
                
                # è·å–2000æ¡å†å²æ•°æ®
                success = initialize_historical_data(symbol, 2000)
                initialization_results[symbol] = success
                
                if success:
                    logger.info(f"âœ“ Successfully initialized historical data for {symbol}")
                else:
                    logger.warning(f"âš  Limited initialization for {symbol} - will accumulate data over time")
                
                # ç¬¦å·é—´ç­‰å¾…5ç§’ï¼ˆé¿å…Binanceæ¥å£é™åˆ¶ï¼‰
                if symbol != symbols[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªç¬¦å·
                    logger.info("Waiting 5 seconds before next symbol...")
                    time.sleep(5)
                    
            except Exception as e:
                logger.error(f"Error during initialization for {symbol}: {str(e)}")
                initialization_results[symbol] = True  # è®¾ä¸ºTrueï¼Œè®©ç³»ç»Ÿç»§ç»­è¿è¡Œ
        
        successful_inits = sum(1 for success in initialization_results.values() if success)
        total_symbols = len(symbols)
        
        if successful_inits == total_symbols:
            logger.info("âœ“ Historical data initialization completed successfully for all symbols")
        elif successful_inits > 0:
            logger.info(f"âš  Partial initialization success ({successful_inits}/{total_symbols} symbols)")
        else:
            logger.warning("âš  Historical data initialization failed for all symbols")
    
    logger.info("Starting main monitoring loop...")
    
    while True:
        try:
            logger.info("Starting new multi-timeframe analysis cycle...")
            all_trends = {}
            all_insights = {}
            trend_changed = False
            
            # æ¯å¤©æ¸…ç†ä¸€æ¬¡æ—§æ•°æ®å’Œæ£€æŸ¥è¿‡æœŸè¶‹åŠ¿
            current_hour = datetime.now().hour
            if current_hour == 0:  # æ¯å¤©åˆå¤œæ¸…ç†
                logger.info("Performing daily data cleanup...")
                cleanup_old_data()
                check_expired_trends()
                
                # æ‰§è¡Œæ•°æ®è´¨é‡ç›‘æ§
                logger.info("Performing daily data quality monitoring...")
                quality_results = quality_monitor.monitor_data_quality(symbols)
                
                for symbol in symbols:
                    if symbol in quality_results.get('quality_reports', {}):
                        quality_report = quality_results['quality_reports'][symbol]
                        logger.info(f"Data quality report for {symbol}: {quality_report.get('quality_grade', 'F')} ({quality_report.get('overall_score', 0):.1f}%)")
                        
                        # å¦‚æœæ•°æ®è´¨é‡ä½ï¼Œå‘é€è­¦å‘Šé€šçŸ¥
                        if quality_report.get('overall_score', 0) < 70:
                            notification_manager.send_quality_warning(symbol, quality_report)
                            logger.warning(f"Low data quality alert sent for {symbol}")
            
            for i, symbol in enumerate(symbols):
                logger.info(f"Analyzing {symbol} across multiple timeframes...")
                
                # æ¯æ¬¡å¾ªç¯æ·»åŠ 5ç§’å»¶è¿Ÿï¼ˆé¿å…Binanceæ¥å£é™åˆ¶ï¼‰
                if i > 0:
                    logger.info("Waiting 5 seconds before next symbol...")
                    time.sleep(5)
                
                # è·å–æœ€æ–°çš„5åˆ†é’Ÿæ•°æ®å¹¶å­˜å‚¨
                try:
                    # é€šè¿‡APIè·å–æœ€æ–°çš„Kçº¿æ•°æ®
                    latest_klines = api_client.fetch_binance_klines(symbol, '1m', 1)
                    if latest_klines:
                        # å­˜å‚¨æœ€æ–°çš„Kçº¿æ•°æ®åˆ°æ•°æ®åº“
                        success = db_manager.store_historical_klines_bulk(symbol, latest_klines, '1m')
                        if success:
                            logger.info(f"Stored latest 1min data for {symbol}")
                        else:
                            logger.error(f"Failed to store 1min data for {symbol}")
                            continue
                    else:
                        logger.error(f"Failed to get 1min data for {symbol}")
                        continue  # å¦‚æœæ— æ³•è·å–1åˆ†é’Ÿæ•°æ®ï¼Œè·³è¿‡è¿™ä¸ªsymbol
                except Exception as e:
                    logger.error(f"Failed to process 1min data for {symbol}: {str(e)}")
                    continue
                
                # åŸºäºæ•°æ®åº“ä¸­çš„1åˆ†é’Ÿæ•°æ®åˆ†æå¤šä¸ªæ—¶é—´æ¡†æ¶
                symbol_trends, symbol_insights = analyze_multiple_timeframes(symbol)
                all_trends[symbol] = symbol_trends
                all_insights[symbol] = symbol_insights
                
                # æ£€æŸ¥å½“å‰å¸ç§æ˜¯å¦æœ‰è¶‹åŠ¿å˜åŒ–ï¼Œå¦‚æœ‰å˜åŒ–ç«‹å³å‘é€ç‹¬ç«‹é€šçŸ¥
                symbol_trend_changed = False
                changed_timeframes = []
                
                for timeframe in ["5m", "15m", "1h", "4h", "1d"]:
                    current_trend = symbol_trends.get(timeframe, "æœªçŸ¥")
                    # åˆå§‹åŒ–last_trendsä¸­ä¸å­˜åœ¨çš„æ—¶é—´æ¡†æ¶
                    if timeframe not in last_trends[symbol]:
                        last_trends[symbol][timeframe] = "æœªçŸ¥"
                    if current_trend != last_trends[symbol][timeframe]:
                        symbol_trend_changed = True
                        trend_changed = True
                        changed_timeframes.append((timeframe, last_trends[symbol][timeframe], current_trend))
                        logger.info(f"{symbol} [{timeframe}] è¶‹åŠ¿å˜åŒ–: {last_trends[symbol][timeframe]} -> {current_trend}")
                        last_trends[symbol][timeframe] = current_trend
                
                # å¦‚æœå½“å‰å¸ç§æœ‰è¶‹åŠ¿å˜åŒ–ï¼Œç«‹å³å‘é€ç‹¬ç«‹é€šçŸ¥
                if symbol_trend_changed:
                    send_individual_trend_notification(symbol, symbol_trends, symbol_insights, changed_timeframes)
                
                # æ·»åŠ 5ç§’å»¶è¿Ÿï¼ˆé¿å…Binanceæ¥å£é™åˆ¶ï¼‰
                logger.info("Waiting 5 seconds before next symbol...")
                time.sleep(5)
            
            # è®°å½•åˆ†æå‘¨æœŸå®Œæˆï¼ˆä¸ªåˆ«é€šçŸ¥å·²åœ¨å¾ªç¯ä¸­å‘é€ï¼‰
            if trend_changed:
                logger.info("Trend changes detected and individual notifications sent")
            else:
                logger.info("No trend changes detected across all timeframes")
                
            logger.info("Multi-timeframe analysis cycle completed, waiting 5 minutes...")
            
        except Exception as e:
            logger.error(f"Error in main loop: {str(e)}")
            logger.info("Continuing after error...")
            
        time.sleep(300)  # æ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡

if __name__ == "__main__":
    main()